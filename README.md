<div align="center">

# llama.rust   
LLM inference in Rust

</div>

## Components
- [kv-cache](./kv-cache): `KVCache` struct definition
- [all-close](./all-close): `all_close` function
- [scaled-dot-product-attention](./scaled-dot-product-attention): `scaled_dot_product_attention` function  
- [round-multiple](./round-multiple/): `round_multiple` function
- [flash-attention](./flash-attention/): `flash attention` definition  
- [linear-attention](./linear-attention/): `linear attention` definition  
- [multihead-attention](./multihead-attention/): `multihead-attention` definition  
- [paged-attention](./paged-attention/): `paged attention` definition  

## References
- [llama.cpp](https://github.com/ggerganov/llama.cpp)
- [llm](https://github.com/rustformers/llm)
- [vLLM](https://github.com/vllm-project/vllm)  
- [mistral.rs](https://github.com/EricLBuehler/mistral.rs)
- [candle](https://github.com/huggingface/candle)