[workspace]
members = [ 
    "inference",
    "kv-cache",
    "scaled-dot-product-attention",
    "all-close",
    "round-multiple",
    "ollama",

    # Attention
    "multihead-attention",
    "flash-attention",
    "paged-attention",
    "linear-attention",

    # Activation
    "sigmoid"
]

resolver = "2"

[workspace.package]
version = "0.1.0"
edition = "2024"
description = "LLM inference in Rust"
license = "MIT"
repository = "https://github.com/zTgx/llama.rust"
readme = "README.md"
keywords = ["llama", "inference", "llm", "rust"]

[profile.release]
debug = true

[workspace.dependencies]
# llama.rust
kv-cache = { path = "./kv-cache", package = "kv-cache", version = "0.1.0" }
all-close ={ path = "./all-close", package = "all-close", version = "0.1.0"}
scaled-dot-product-attention = { path = "./scaled-dot-product-attention", package = "scaled-dot-product-attention", version = "0.1.0"}

clap = { version = "4.5", features = ["derive"] }
anyhow = "1.0"
candle-core = { version = "0.9.1" }
# candle-core = { version = "0.9.1", features = ["cuda"] }
candle-transformers = { version = "0.9.1" }
candle-nn = "0.9.1"
polars = "0.49.1"
ndarray = "0.15"
ndarray-rand = "0.14"
rand = "0.8"
rand_isaac = "0.3"
tokenizers = { version = "0.21.2", features = ["http"] }
hf-hub = "0.4.1"
tracing-chrome = "0.7.1"
tracing-subscriber = "0.3.7"
serde_json = "1.0.99"
ollama-rs = "0.3.2"
chrono = { version = "0.4", features = ["serde"] }

