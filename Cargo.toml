[package]
name = "llama-rust"
version = "0.1.0"
edition = "2024"
description = "LLM inference in Rust"
license = "MIT"
repository = "https://github.com/zTgx/llama.rust"
readme = "README.md"
keywords = ["llama", "inference", "ggml-rs"]

[dependencies]
clap = { version = "4.5", features = ["derive"] }
anyhow = "1.0"
candle-core = { version = "0.9.1", features = ["cuda"] }
candle-nn = "0.9.1"
polars = "0.49.1"
ndarray = "0.15"
ndarray-rand = "0.14"
rand = "0.8"
rand_isaac = "0.3"
tokenizers = "0.21.2"