[workspace]
members = [ 
    "inference",
    "llama"
]

resolver = "2"

[workspace.package]
version = "0.6.0"
edition = "2024"
description = "LLM inference in Rust"
license = "MIT"
repository = "https://github.com/zTgx/llama.rust"
readme = "README.md"
keywords = ["llama", "inference", "llm", "rust"]

[profile.release]
debug = true

[workspace.dependencies]
clap = { version = "4.5", features = ["derive"] }
anyhow = "1.0"
candle-core = { version = "0.9.1", features = ["cuda"] }
candle-transformers = { version = "0.9.1" }
candle-nn = "0.9.1"
polars = "0.49.1"
ndarray = "0.15"
ndarray-rand = "0.14"
rand = "0.8"
rand_isaac = "0.3"
tokenizers = { version = "0.21.2", features = ["http"] }
hf-hub = "0.4.1"
tracing-chrome = "0.7.1"
tracing-subscriber = "0.3.7"
serde_json = "1.0.99"
